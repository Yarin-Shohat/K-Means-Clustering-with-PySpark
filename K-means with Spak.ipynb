{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea933ca9-1532-4406-b540-e6926a62824d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dead102d-5dd2-4a74-990c-c423a878d5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def minMax_Scale(data):\n",
    "    \"\"\"\n",
    "    Function to Min Max Scale the Data using sklearn MinMaxScaler.\n",
    "\n",
    "    :param data: (pyspark.sql.dataframe.DataFrame) The data.\n",
    "    :return: pyspark.sql.dataframe.DataFrame: Normalized Data\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    # Cast to Pandas DataFrame\n",
    "    pandas_df = data.toPandas()\n",
    "    # Scale\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(pandas_df), columns=pandas_df.columns)\n",
    "    # Slice off the last column and convert back to a Spark DataFrame\n",
    "    scaled_data = scaled_data.iloc[:, :-1]\n",
    "    scaled_spark = spark.createDataFrame(scaled_data)\n",
    "    return scaled_spark\n",
    "\n",
    "\n",
    "def Kmeans(data, k, ct=0.0001, iterations=30, initial_centroids=None):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering using MapReduce.\n",
    "\n",
    "    Parameters:\n",
    "        data (pyspark.sql.dataframe.DataFrame): A dataset in pyspark.sql.dataframe.DataFrame format\n",
    "        k: the number of clusters\n",
    "        ct: Convergence threshold (parameter - default is set to 0.0001)\n",
    "        iterations: Number of iteration per experiment (parameter - default is set to 30)\n",
    "        initial_centroids (list): - List of initial centroid locations where each centroid is represented by a tuple of the location\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the centroids calculated by the algorithm so that each centroid is represented by a tuple of its location\n",
    "    \"\"\"\n",
    "    # Check input\n",
    "    if data is None or initial_centroids == [] or k <=0:\n",
    "        return initial_centroids\n",
    "    # Min Max Scale\n",
    "    data = minMax_Scale(data)\n",
    "    # Cast to RDD of NumPy arrays\n",
    "    rdd = data.rdd.map(lambda row: np.array(row))\n",
    "\n",
    "    # Initialize centroids if not entered by the user by takeSample(False, k)\n",
    "    if initial_centroids is None:\n",
    "        centroids = rdd.takeSample(False, k)\n",
    "    else:\n",
    "        centroids = initial_centroids\n",
    "\n",
    "    # Preform Iterations\n",
    "    for i in range(iterations):\n",
    "        # Function for the Map step\n",
    "        def Map_Step(point):\n",
    "            \"\"\"\n",
    "            Assign points to the nearest centroid\n",
    "\n",
    "            Parameters:\n",
    "                point: point to assign to new cluster\n",
    "\n",
    "            Returns:\n",
    "                nearest centroid of the point\n",
    "            \"\"\"\n",
    "            distances = [np.linalg.norm(point - centroid) for centroid in centroids]\n",
    "            return np.argmin(distances), point\n",
    "        \n",
    "        # Reduce - Recalculate centroids as the mean of assigned points\n",
    "        def Reduce_Step(point1, point2):\n",
    "            \"\"\"\n",
    "            Combines two points\n",
    "\n",
    "            Parameters:\n",
    "                point1: First point\n",
    "                point2: Second point\n",
    "\n",
    "            Returns:\n",
    "                points combined\n",
    "            \"\"\"\n",
    "            return point1[0] + point2[0], point1[1] + point2[1]\n",
    "        \n",
    "        # Send to all workers\n",
    "        bc_centroids = sc.broadcast(centroids)\n",
    "\n",
    "        # For each data point xi: Find the nearest centroid and Assign the point to that cluster\n",
    "        temp_clustered_points = rdd.map(Map_Step)\n",
    "\n",
    "        # Calculate new centroids\n",
    "        new_centroids = (\n",
    "            temp_clustered_points\n",
    "            .mapValues(lambda point: (point, 1))  # Map step on points\n",
    "            .reduceByKey(Reduce_Step)  # Reduce Step\n",
    "            .mapValues(lambda x: x[0] / x[1])  # Calculate the mean for each cluster\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        # For each cluster j=1,â€¦,k: new centroid = average of all points assigned to cluster c\n",
    "        new_centroids = [centroid[1] for centroid in sorted(new_centroids)]\n",
    "\n",
    "        # Check if the change in centroid positions is less than the specified threshold (ct).\n",
    "        check_converged = True\n",
    "        for new, old in zip(new_centroids, centroids):\n",
    "            if np.linalg.norm(np.array(new) - np.array(old)) >= ct:\n",
    "                check_converged = False\n",
    "                break\n",
    "\n",
    "        # Check that we aren't missing Clusters(Merge or other reason)\n",
    "        if len(new_centroids) < k:\n",
    "            missing_clusters = k - len(new_centroids)\n",
    "            # Centroids that exist\n",
    "            existing_points = set(map(tuple, new_centroids))\n",
    "            all_points = set(map(tuple, rdd.collect()))\n",
    "            # New centroids\n",
    "            new_random_centroids = rdd.takeSample(False, missing_clusters)\n",
    "            new_centroids.extend(new_random_centroids)\n",
    "            check_converged = False\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "        # Break if the change in centroid positions is less than the specified threshold (ct).\n",
    "        if check_converged:\n",
    "            break\n",
    "    \n",
    "    final_centroids = [tuple(round(num, 5) for num in tup) for tup in centroids]\n",
    "    return final_centroids"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hw4_ 211987987",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
